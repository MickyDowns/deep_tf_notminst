{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 5\n",
    "------------\n",
    "\n",
    "The goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding vectors of words: https://www.tensorflow.org/versions/0.6.0/tutorials/word2vec/index.html\n",
    "\n",
    "General\n",
    "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of the individual raw pixel-intensities for image data, or e.g. power spectral density coefficients for audio data. For tasks like object or speech recognition we know that all the information required to successfully perform the task is encoded in the data (because humans can perform these tasks from the raw data). However, natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about 'cats' when it is processing data about 'dogs' (such that they are both animals, four-legged, pets, etc.). Representing words as unique, discrete ids furthermore leads to data sparsity, and usually means that we may need more data in order to successfully train statistical models. Using vector representations can overcome some of these obstacles.\n",
    "\n",
    "Embedding\n",
    "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: count-based methods (e.g. Latent Semantic Analysis), and predictive methods (e.g. neural probabilistic language models).\n",
    "\n",
    "Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word (RIGHT, THINK \"THESE ARE THE WORDS THAT MOST COMMONLY SURROUND THE TARGET WORK\". SO, IF TWO TARGET WORDS HAVE SIMILAR WORD VECTORS, THEY HAVE SIMILAR MEANINGS). Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model) (I.E., THINK \"THESE WORDS PREDICT THE TARGET WORK\"\n",
    "\n",
    "Word2Vec, CBOW and Skip-Gram\n",
    "Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. Algorithmically, these models are similar, except that CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). (RIGHT, THIS IS THE SUMMING IN THE ASSIGNMENT). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the continuous bag of words and skip-gram architectures, in layman's terms?\n",
    "\n",
    "Both architectures describe how the neural network \"learns\" the underlying word representations for each word. Since learning word representations is essentially unsupervised, you need some way to \"create\" labels to train the model. Skip-gram and CBOW are two ways of creating the \"task\" for the neural network -- you can think of this as the output layer of the neural network, where we create \"labels\" for the given input (which depends on the architecture).\n",
    "\n",
    "For both descriptions below, we assume that the current word in a sentence is wi.\n",
    "\n",
    "Skip-gram: The input to the model is wi, and the output could be wi−1,wi−2,wi+1,wi+2. So the task here is \"predicting the context given a word\". Also, the context is not limited to its immediate context, training instances can be created by skipping a constant number of words in its context, so for example, wi−3,wi−4,wi+3,wi+4, hence the name skip-gram. Note that the window size determines how far forward and backward to look for context words to predict.\n",
    "\n",
    "CBOW: The input to the model could be wi−2,wi−1,wi+1,wi+2, the preceding and following words of the current word we are at. The output of the neural network will be wi. Hence you can think of the task as \"predicting the word given its context\". Note that the number of words we use depends on your setting for the window size.\n",
    "\n",
    "According to Mikolov:\n",
    "Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.\n",
    "\n",
    "CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
    "\n",
    "This can get even a bit more complicated if you consider that there are two different ways how to train the models: the normalized hierarchical softmax, and the un-normalized negative sampling. Both work quite differently.\n",
    "which makes sense since with skip gram, you can create a lot more training instances from limited amount of data, and for CBOW, you will need more since you are conditioning on context, which can get exponentially huge.\n",
    "\n",
    "https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "0K1ZyLn04QZf"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCjPJE944bkV"
   },
   "source": [
    "Download the data from the source website if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 14640,
     "status": "ok",
     "timestamp": 1445964482948,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "c4ec222c-80b5-4298-e635-93ca9f79c3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zqz3XiqI4mZT"
   },
   "source": [
    "Read the data into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28844,
     "status": "ok",
     "timestamp": 1445964497165,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "e3a928b4-1645-4fe8-be17-fcf47de5716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28849,
     "status": "ok",
     "timestamp": 1445964497178,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "3fb4ecd1-df67-44b6-a2dc-2291730970b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]] # creating a vector of tuples associating word with its count\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    # this is using \"collections\" class to count occurrances of each word and order greatest to least.\n",
    "    # collections is a \"high performance\" data type (as opposed to \"built-in\" containders like dict, list, tuple)\n",
    "    # so, count ends up being the most frequently occurring 50k words in decreasing order of occurrence. \n",
    "    \n",
    "    dictionary = dict() # creating an empty dictionary ready to store key-value pair\n",
    "    for word, _ in count: # working from most common to least\n",
    "        dictionary[word] = len(dictionary) # adding words to dictionary using word as key and counter as value\n",
    "    # dictionary ends up being a list of tuples indexed by the word w/ value = rank ordinal of occurrence\n",
    "    \n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary: # now, running back through original list of words\n",
    "            index = dictionary[word] # creating index entry cross referencing input word to its dictionary entry\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "        # data ends up being string of numeric index values 1 for each word in original words input file\n",
    "        # each data value (index integer value) represents the commonness of the word w/ 1 being most common\n",
    "        # finally, for words < 50,0000 commonness rank, their index is 0 (total of 419k)\n",
    "        \n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    # swapping dictionary key and value columns so that the number becomes the key and the word becomes the value\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "# so, the dataset is a list including:\n",
    "#  1. data: numeric representation of \"words\" file\n",
    "#  2. count: the words and their frequency of occurrance sorted by frequency\n",
    "#  3. dictionary: \"word\" as key, ordinal for frequency of occurrance as value\n",
    "#  4. reverse dictionary: ordinal for foo as key and word as value\n",
    "\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n",
      "[['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873)]\n",
      "6\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(data[1:10])\n",
    "print(count[0:7])\n",
    "print(dictionary['a'])\n",
    "print(reverse_dictionary[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/2/library/collections.html\n",
    "\n",
    "class collections.Counter([iterable-or-mapping])\n",
    "A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.\n",
    "\n",
    "Elements are counted from an iterable or initialized from another mapping (or counter):\n",
    ">>> c = Counter()                           # a new, empty counter\n",
    ">>> c = Counter('gallahad')                 # a new counter from an iterable\n",
    ">>> c = Counter({'red': 4, 'blue': 2})      # a new counter from a mapping\n",
    ">>> c = Counter(cats=4, dogs=8)             # a new counter from keyword args\n",
    "\n",
    "Counter objects have a dictionary interface except that they return a zero count for missing items instead of raising a KeyError:\n",
    ">>> c = Counter(['eggs', 'ham'])\n",
    ">>> c['bacon']                              # count of a missing element is zero\n",
    "0\n",
    "\n",
    "Setting a count to zero does not remove an element from a counter. Use del to remove it entirely:\n",
    ">>> c['sausage'] = 0                        # counter entry with a zero count\n",
    ">>> del c['sausage']                        # del actually removes the entry\n",
    "\n",
    "most_common([n])\n",
    "Return a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered arbitrarily:\n",
    ">>> Counter('abracadabra').most_common(3)\n",
    "[('a', 5), ('r', 2), ('b', 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the skip-gram model.\n",
    "\n",
    "https://www.tensorflow.org/versions/0.6.0/tutorials/word2vec/index.html\n",
    "As an example, let's consider the dataset\n",
    "\n",
    "\"the quick brown fox jumped over the lazy dog\"\n",
    "\n",
    "We first form a dataset of words and the contexts in which they appear. We could define 'context' in any way that makes sense, and in fact people have looked at syntactic contexts (i.e. the syntactic dependents of the current target word, see e.g. Levy et al.), words-to-the-left of the target, words-to-the-right of the target, etc. For now, let's stick to the vanilla definition and define 'context' as the window of words to the left and to the right of a target word. Using a window size of 1, we then have the dataset\n",
    "\n",
    "([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...\n",
    "\n",
    "of (context, target) pairs. Recall that skip-gram inverts contexts and targets, and tries to predict each context word from its target word, so the task becomes to predict 'the' and 'brown' from 'quick', 'quick' and 'fox' from 'brown', etc. Therefore our dataset becomes\n",
    "\n",
    "(quick, the), (quick, brown), (brown, quick), (brown, fox), ...\n",
    "\n",
    "of (input, output) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1445964901989,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w9APjA-zmfjV",
    "outputId": "67cccb02-cdaf-4e47-d489-43bcc8d57bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['as', 'anarchism', 'originated', 'a', 'as', 'term', 'of', 'a']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
      "    labels: ['a', 'term', 'anarchism', 'originated', 'as', 'of', 'originated', 'term']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "# THIS IS WHERE SKIP-GRAM (VS. BOW) IS IMPLEMENTED. IF SG IS PREDICT CONTEXT GIVEN WORD AND BOW IS PREDICT WORD\n",
    "# GIVEN CONTEXT, THEN WE NEED TO REVERSE BATCH AND LABEL PROCESSING BELOW. \n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window # setting debug parameters\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32) # initilizing batch and label vectors\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ] # defines window of words we grab each time\n",
    "    buffer = collections.deque(maxlen=span) \n",
    "    # defines fixed length word bag in which to iteratively \"pop\" each bag of words \n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index]) # create series of word buffers by \"popping\" off sets of n words\n",
    "        data_index = (data_index + 1) % len(data) # increment data index\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips): # seemingly complex way to grab words before and after\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]: # generating series of skip/window tuple parms i.e., [2,4] and [1,2]\n",
    "    # [2,1] means we will predict each word based on the word immediately before and immediately after\n",
    "    # [4,2] means we will predict each word based on the two words immediately before and after\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3. The assert statement\n",
    "Assert statements are a convenient way to insert debugging assertions into a program:\n",
    "\n",
    "assert_stmt ::=  \"assert\" expression [\",\" expression]\n",
    "The simple form, assert expression, is equivalent to\n",
    "\n",
    "if __debug__:\n",
    "   if not expression: raise AssertionError\n",
    "The extended form, assert expression1, expression2, is equivalent to\n",
    "\n",
    "if __debug__:\n",
    "   if not expression1: raise AssertionError(expression2)\n",
    "These equivalences assume that __debug__ and AssertionError refer to the built-in variables with those names. In the current implementation, the built-in variable __debug__ is True under normal circumstances, False when optimization is requested (command line option -O). The current code generator emits no code for an assert statement when optimization is requested at compile time. Note that it is unnecessary to include the source code for the expression that failed in the error message; it will be displayed as part of the stack trace.\n",
    "\n",
    "Assignments to __debug__ are illegal. The value for the built-in variable is determined when the interpreter starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3.2. deque objects\n",
    "\n",
    "class collections.deque([iterable[, maxlen]])\n",
    "\n",
    "Returns a new deque object initialized left-to-right (using append()) with data from iterable. If iterable is not specified, the new deque is empty.\n",
    "\n",
    "Deques are a generalization of stacks and queues (the name is pronounced “deck” and is short for “double-ended queue”). Deques support thread-safe, memory efficient appends and pops from either side of the deque with approximately the same O(1) performance in either direction.\n",
    "\n",
    "Though list objects support similar operations, they are optimized for fast fixed-length operations and incur O(n) memory movement costs for pop(0) and insert(0, v) operations which change both the size and position of the underlying data representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ofd1MbBuwiva"
   },
   "source": [
    "Train a skip-gram model.\n",
    "\n",
    "https://www.tensorflow.org/versions/0.6.0/tutorials/word2vec/index.html\n",
    "\n",
    "The objective function is defined over the entire dataset, but we typically optimize this with stochastic gradient descent (SGD) using one example at a time (or a 'minibatch' of batch_size examples, where typically 16 <= batch_size <= 512). So let's look at one step of this process.\n",
    "\n",
    "Let's imagine at training step \"t\" we observe the first training case above, where the goal is to predict the from quick. We select num_noise number of noisy (contrastive) examples by drawing from some noise distribution, typically the unigram distribution, P(w). For simplicity let's say num_noise=1 and we select sheep as a noisy example. Next we compute the loss for this pair of observed and noisy examples, i.e. the objective at time step t becomes: \n",
    "     J_NEG^t = log Q_θ(D=1|the, quick) + log Q_θ(D=0|sheep, quick).\n",
    "\n",
    "The goal is to make an update to the embedding parameters θ to improve (in this case, maximize) this objective function. We do this by deriving the gradient of the loss with respect to the embedding parameters θ, i.e. ∂/∂θ J_NEG. We then perform an update to the embeddings by taking a small step in the direction of the gradient (i.e., θ_t<-θ_t-1 + gradient step) . When this process is repeated over the entire training set, this has the effect of 'moving' the embedding vectors around for each word until the model is successful at discriminating real words from noise words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/versions/0.6.0/tutorials/word2vec/index.html\n",
    "\n",
    "This is all about embeddings, so let's define our embedding matrix. This is just a big random matrix to start. We'll initialize the values to be uniform in the unit cube.\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "The noise-contrastive estimation loss is defined in terms a logistic regression model. For this, we need to define the weights and biases for each word in the vocabulary (also called the output weights as opposed to the input embeddings). So let's define that.\n",
    "\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "Now that we have the parameters in place, we can define our skip-gram model graph. For simplicity, let's suppose we've already integerized our text corpus with a vocabulary so that each word is represented as an integer (see tensorflow/examples/tutorials/word2vec/word2vec_basic.py for the details). The skip-gram model takes two inputs. One is a batch full of integers representing the source context words, the other is for the target words. Let's create placeholder nodes for these inputs, so that we can feed in data later.\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# skip-gram looks up vector for each source (context) word in batch\n",
    "Now what we need to do is look up the vector for each of the source words in the batch. TensorFlow has handy helpers that make this easy.\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs) \n",
    "\n",
    "train_inputs is the input batch of context word and target. so, this is getting the embedding vect for each. Ok, now that we have the embeddings for each word, we'd like to try to predict the target word using the noise-contrastive training objective. Compute the NCE loss, using a sample of the negative labels each time.\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,num_sampled, vocabulary_size))\n",
    "\n",
    "Now that we have a loss node, we need to add the nodes required to compute gradients and update the parameters, etc. For this we will use stochastic gradient descent, and TensorFlow has handy helpers to make this easy as well. So, using the SGD optimizer:\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8pQKsV4Vwlzy"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # initializing embedding matrix of 50,000 (all words in the dictionary (of top 50000))\n",
    "    # x 128 which is the length of all individual embeddings end-to-end\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        # each word has 128 weights. one for each word associated with it.\n",
    "    \n",
    "    # initializing output(?) weights and biases\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        # so, each word has a bias value associated w/ it.\n",
    "    \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # finding the latest embed values for the input batch of words\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                                                     train_labels, num_sampled, vocabulary_size))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) # optimize given knowledge of data strcuture\n",
    "    \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    # above creates matrix of the sqrt of the sum of all the embeddings\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "    # so, similarity is mathmatically calculated vs. all other embeddings using matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)\n",
    "\n",
    "Outputs random values from a uniform distribution.\n",
    "\n",
    "The generated values follow a uniform distribution in the range [minval, maxval). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n",
    "\n",
    "For floats, the default range is [0, 1). For ints, at least maxval must be specified explicitly.\n",
    "\n",
    "In the integer case, the random integers are slightly biased unless maxval - minval is an exact power of two. The bias is small for values of maxval - minval significantly smaller than the range of the output (either 2**32 or 2**64).\n",
    "\n",
    "Args:\n",
    "\n",
    "shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
    "minval: A 0-D Tensor or Python value of type dtype. The lower bound on the range of random values to generate. Defaults to 0.\n",
    "maxval: A 0-D Tensor or Python value of type dtype. The upper bound on the range of random values to generate. Defaults to 1 if dtype is floating point.\n",
    "dtype: The type of the output: float32, float64, int32, or int64.\n",
    "seed: A Python integer. Used to create a random seed for the distribution. See set_random_seed for behavior.\n",
    "name: A name for the operation (optional).\n",
    "Returns:\n",
    "\n",
    "A tensor of the specified shape filled with random uniform values.\n",
    "\n",
    "***\n",
    "\n",
    "embedding_lookup function retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy. E.g.\n",
    "\n",
    "matrix = np.random.random([1024, 64])  # 64-dimensional embeddings\n",
    "ids = np.array([0, 5, 17, 33])\n",
    "print matrix[ids]  # prints a matrix of shape [4, 64] \n",
    "params argument can be also a list of tensors in which case the ids will be distributed among the tensors. E.g. given a list of 3 [2, 64] tensors the default behavior is that they will represent ids: [0, 3], [1, 4], [2, 5]. partition_strategy controls the way how the ids are distributed among the list. The partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.\n",
    "\n",
    "*** \n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization: We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic op- timization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal func- tion, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu- larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/versions/0.6.0/tutorials/word2vec/index.html\n",
    "    \n",
    "Training the Model\n",
    "\n",
    "Training the model is then as simple as using a feed_dict to push data into the placeholders and calling session.run with this new data in a loop.\n",
    "\n",
    "for inputs, labels in generate_batch(...):\n",
    "  feed_dict = {training_inputs: inputs, training_labels: labels}\n",
    "  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "See the full example code in tensorflow/examples/tutorials/word2vec/word2vec_basic.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 23
      },
      {
       "item_id": 48
      },
      {
       "item_id": 61
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 436189,
     "status": "ok",
     "timestamp": 1445965429787,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "1bQFGceBxrWW",
    "outputId": "5ebd6d9a-33c6-4bcd-bf6d-252b0b6055e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 7.579554\n",
      "Nearest to new: osborn, whore, biblioth, salaam, bikini, oc, wiesenthal, ills,\n",
      "Nearest to more: negatives, fiercest, foosball, twenty, demille, fingernails, tm, reintroduction,\n",
      "Nearest to were: screwed, lukas, kauravas, transcontinental, aerosol, rajons, housekeeping, disapproval,\n",
      "Nearest to to: dragged, duck, opponents, polanyi, verisimilitude, cauldron, actium, triadic,\n",
      "Nearest to two: hoff, pitcher, delicacies, clipperton, conjunct, opined, protocols, trajectories,\n",
      "Nearest to american: hostage, tuskegee, beatle, sponsored, asian, opposition, unharmed, called,\n",
      "Nearest to he: pedestrian, empiricism, publicized, articulated, johansson, loxodonta, consignee, invitational,\n",
      "Nearest to war: recorders, post, tradeoff, immortal, costumes, ediacaran, frustrations, dispensation,\n",
      "Nearest to state: leave, translational, capitalize, eon, dumfries, philosophiae, stillborn, asgard,\n",
      "Nearest to who: configuration, swap, borrelly, weizmann, nmi, indifferent, regains, rank,\n",
      "Nearest to is: layer, nasty, boundary, cumin, busey, ethnographic, sergius, epicurean,\n",
      "Nearest to d: regulate, instrumentation, controller, khanum, hire, lessened, mysticism, authorization,\n",
      "Nearest to after: litovsk, beethoven, cashier, divide, elgin, enquiry, cga, truss,\n",
      "Nearest to nine: tonnage, begin, foraminifera, egoism, quantification, stencil, steamboat, jacobites,\n",
      "Nearest to and: altruist, fiorello, geer, leonel, best, casanova, peacetime, passer,\n",
      "Nearest to there: carbamazepine, herbaceous, negus, lollardy, oct, augustin, astrodome, cleaned,\n",
      "Average loss at step 5000: 4.050829\n",
      "Average loss at step 10000: 3.717846\n",
      "Average loss at step 15000: 3.582125\n",
      "Average loss at step 20000: 3.574093\n",
      "Nearest to new: alga, istv, psychical, biblioth, cycle, salaam, television, alle,\n",
      "Nearest to more: less, most, smaller, rebbe, validating, gemara, themselves, constitutional,\n",
      "Nearest to were: are, was, have, had, be, been, examine, by,\n",
      "Nearest to to: will, for, in, can, would, not, mfm, agave,\n",
      "Nearest to two: three, four, six, eight, five, seven, one, nine,\n",
      "Nearest to american: dunkirk, malvinas, perilous, british, outings, da, massif, english,\n",
      "Nearest to he: she, it, they, who, there, eventually, pedestrian, later,\n",
      "Nearest to war: sdram, conway, tracking, banking, beaver, post, costumes, poem,\n",
      "Nearest to state: leave, nay, kicker, regia, sum, protest, appearing, diaeresis,\n",
      "Nearest to who: he, surmounted, also, they, that, cfcs, which, beowulf,\n",
      "Nearest to is: was, are, has, as, single, chau, be, undergraduates,\n",
      "Nearest to d: b, regulate, pints, controller, instrumentation, mysticism, accessed, goodwin,\n",
      "Nearest to after: at, when, for, during, before, divide, brill, reflecting,\n",
      "Nearest to nine: eight, five, seven, six, four, three, zero, two,\n",
      "Nearest to and: or, of, including, in, but, hillary, telephones, s,\n",
      "Nearest to there: they, it, he, hydrolysis, still, she, headlining, never,\n",
      "Average loss at step 25000: 3.484188\n",
      "Average loss at step 30000: 3.423007\n",
      "Average loss at step 35000: 3.396507\n",
      "Average loss at step 40000: 3.468359\n",
      "Nearest to new: ainu, alga, laymen, special, istv, modern, regular, stun,\n",
      "Nearest to more: less, most, very, smaller, validating, pseudolus, highly, whistle,\n",
      "Nearest to were: are, was, had, have, be, been, when, although,\n",
      "Nearest to to: would, will, could, speller, palaeologus, must, can, hengest,\n",
      "Nearest to two: five, three, zero, four, six, seven, one, eight,\n",
      "Nearest to american: british, english, malvinas, leto, metamorphoses, gloucestershire, slender, massif,\n",
      "Nearest to he: she, it, they, who, there, later, then, but,\n",
      "Nearest to war: beaver, sdram, claude, tropospheric, purpose, dispensation, mundi, post,\n",
      "Nearest to state: protest, regia, leave, jurist, peculiarities, court, veterans, kicker,\n",
      "Nearest to who: he, they, which, she, that, surmounted, it, beowulf,\n",
      "Nearest to is: was, has, are, draugr, be, nextel, makes, tuples,\n",
      "Nearest to d: b, regulate, ch, baths, gillian, promoted, j, woodside,\n",
      "Nearest to after: before, when, during, for, afghana, if, through, despite,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, two, three,\n",
      "Nearest to and: or, but, which, tragically, warn, crochet, cavers, enzyte,\n",
      "Nearest to there: they, it, he, she, we, which, hydrolysis, still,\n",
      "Average loss at step 45000: 3.356020\n",
      "Average loss at step 50000: 3.434398\n",
      "Average loss at step 55000: 3.434099\n",
      "Average loss at step 60000: 3.346908\n",
      "Nearest to new: istv, previous, ainu, special, alga, laymen, awkward, auchinleck,\n",
      "Nearest to more: less, most, very, smaller, highly, validating, longer, gemara,\n",
      "Nearest to were: are, was, had, although, have, being, been, be,\n",
      "Nearest to to: could, would, shall, should, palaeologus, against, will, chittagong,\n",
      "Nearest to two: three, five, eight, four, seven, six, one, nine,\n",
      "Nearest to american: british, italian, english, african, french, australian, metamorphoses, european,\n",
      "Nearest to he: she, it, they, there, who, pollinators, we, never,\n",
      "Nearest to war: wars, beaver, clays, purpose, rounders, demyelinating, dispensation, post,\n",
      "Nearest to state: regia, mortals, protest, unwise, jurist, court, leave, match,\n",
      "Nearest to who: also, which, he, they, often, she, generally, beowulf,\n",
      "Nearest to is: was, has, are, be, does, became, nextel, although,\n",
      "Nearest to d: b, l, j, ch, c, ilmarinen, o, r,\n",
      "Nearest to after: before, during, when, despite, at, while, though, although,\n",
      "Nearest to nine: eight, seven, six, five, four, three, zero, two,\n",
      "Nearest to and: or, but, quintessence, s, pacino, writeup, alcal, in,\n",
      "Nearest to there: they, he, it, she, still, we, usually, hydrolysis,\n",
      "Average loss at step 65000: 3.376214\n",
      "Average loss at step 70000: 3.384968\n",
      "Average loss at step 75000: 3.386083\n",
      "Average loss at step 80000: 3.312134\n",
      "Nearest to new: endorsed, previous, istv, certain, special, krona, miao, bouvier,\n",
      "Nearest to more: less, most, very, greater, longer, smaller, relatively, rather,\n",
      "Nearest to were: are, was, had, have, those, although, while, included,\n",
      "Nearest to to: can, will, should, would, might, could, must, hengest,\n",
      "Nearest to two: three, one, seven, five, four, six, eight, zero,\n",
      "Nearest to american: british, african, english, australian, french, indian, metamorphoses, european,\n",
      "Nearest to he: she, it, they, there, we, who, never, pollinators,\n",
      "Nearest to war: wars, clays, beaver, rounders, demyelinating, mundi, purpose, configure,\n",
      "Nearest to state: gym, diaeresis, pastiches, match, load, jurist, city, southerly,\n",
      "Nearest to who: she, surmounted, never, he, they, which, also, often,\n",
      "Nearest to is: was, has, are, be, provides, shameful, draugr, were,\n",
      "Nearest to d: b, ch, l, squeak, j, p, claris, c,\n",
      "Nearest to after: before, during, despite, when, while, without, until, rafik,\n",
      "Nearest to nine: eight, seven, six, five, four, three, zero, one,\n",
      "Nearest to and: or, of, while, including, executables, but, like, writeup,\n",
      "Nearest to there: they, it, he, she, usually, we, often, still,\n",
      "Average loss at step 85000: 3.322993\n",
      "Average loss at step 90000: 3.292691\n",
      "Average loss at step 95000: 2.935105\n",
      "Average loss at step 100000: 3.224013\n",
      "Nearest to new: istv, ainu, endorsed, bouvier, manuals, previous, disused, old,\n",
      "Nearest to more: less, most, very, longer, greater, smaller, rather, highly,\n",
      "Nearest to were: are, was, those, have, while, be, had, these,\n",
      "Nearest to to: can, should, orphan, must, might, would, could, may,\n",
      "Nearest to two: three, four, five, six, seven, eight, one, zero,\n",
      "Nearest to american: british, english, australian, french, irish, canadian, japanese, german,\n",
      "Nearest to he: she, it, they, there, who, but, we, never,\n",
      "Nearest to war: wars, demyelinating, rounders, costumes, heron, beaver, probabilistic, thai,\n",
      "Nearest to state: states, city, match, load, party, pastiches, diaeresis, gym,\n",
      "Nearest to who: which, often, also, she, now, he, they, that,\n",
      "Nearest to is: was, has, although, holds, does, are, appears, be,\n",
      "Nearest to d: b, unreasonable, emotive, stink, instrumentation, april, patriarchate, ilmarinen,\n",
      "Nearest to after: before, while, during, when, despite, following, although, without,\n",
      "Nearest to nine: eight, seven, six, four, five, three, zero, two,\n",
      "Nearest to and: but, or, including, while, like, however, which, molarity,\n",
      "Nearest to there: it, they, she, still, we, often, he, usually,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        # run thru the data getting a 8 word (?) batch\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        # grab the corresponding records from the train dataset\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        # increment average loss by latest loss\n",
    "        \n",
    "        # every 5,000, test against validation set\n",
    "        if step % 5000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 5000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        \n",
    "        # every 20,000, provide the validation word, and the 8 nearest neighbors\n",
    "        if step % 20000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "            # to be clear, we're showing the nearest in the results space, not the current input record.\n",
    "            # that's how, w/ skip-gram [1,2], we go from:\n",
    "            #    Nearest to american: fellow, tyre, invasive, tyrosine, dignities, atta, floss, villa \n",
    "            # to: \n",
    "            #    Nearest to american: british, canadian, italian, ocampo, japanese, australian, barns, russian\n",
    "            # and w/ skip-gram [2,4], we go from:\n",
    "            #    Nearest to american: hostage, tuskegee, beatle, sponsored, asian, opposition, unharmed, called,\n",
    "            # to: \n",
    "            #    Nearest to american: british, english, australian, french, irish, canadian, japanese, german,\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNE\n",
    "We can visualize the learned vectors by projecting them down to 2 dimensions using for instance something like the t-SNE dimensionality reduction technique. When we inspect these visualizations it becomes apparent that the vectors capture some general, and in fact quite useful, semantic information about words and their relationships to one another. It was very interesting when we first discovered that certain directions in the induced vector space specialize towards certain semantic relationships, e.g. male-female, gender and even country-capital relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jjJXYA_XzV79"
   },
   "outputs": [],
   "source": [
    "num_points = 400\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1445965465525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "o_e0D_UezcDe",
    "outputId": "df22e4a5-e8ec-4e5e-d384-c6cf37c68c34"
   },
   "outputs": [],
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how's it used:\n",
    "\n",
    "Evaluating Embeddings: Analogical Reasoning\n",
    "\n",
    "Embeddings are useful for a wide variety of prediction tasks in NLP. Short of training a full-blown part-of-speech model or named-entity model, one simple way to evaluate embeddings is to directly use them to predict syntactic and semantic relationships like king is to queen as father is to ?. This is called analogical reasoning and the task was introduced by Mikolov and colleagues, and the dataset can be downloaded from here: https://word2vec.googlecode.com/svn/trunk/questions-words.txt.\n",
    "\n",
    "To see how we do this evaluation, have a look at the build_eval_graph() and eval() functions in tensorflow/models/embedding/word2vec.py.\n",
    "\n",
    "The choice of hyperparameters can strongly influence the accuracy on this task. To achieve state-of-the-art performance on this task requires training over a very large dataset, carefully tuning the hyperparameters and making use of tricks like subsampling the data, which is out of the scope of this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB5EFrBnpNnc"
   },
   "source": [
    "---\n",
    "\n",
    "Problem\n",
    "-------\n",
    "\n",
    "An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return f.read(name).split()\n",
    "  f.close()\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "5239 -> 3084\n",
      "anarchism -> originated\n",
      "12 -> 3084\n",
      "as -> originated\n",
      "6 -> 12\n",
      "a -> as\n",
      "3084 -> 12\n",
      "originated -> as\n",
      "195 -> 6\n",
      "term -> a\n",
      "12 -> 6\n",
      "as -> a\n",
      "6 -> 195\n",
      "a -> term\n",
      "2 -> 195\n",
      "of -> term\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "# THIS IS WHERE SKIP-GRAM (VS. CBOW) IS IMPLEMENTED. IF SG IS PREDICT CONTEXT GIVEN WORD AND CBOW IS PREDICT WORD\n",
    "# GIVEN SUM CONTEXT, THEN WE NEED TO:\n",
    "# 1. REVERSE BATCH AND LABEL PROCESSING BELOW\n",
    "# 2. CHANGE LOSS(?) FUNCTION TO SUM EMBEDDINGS\n",
    "\n",
    "def generate_batch_cbow(batch_size, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % context_window == 0 # ensure the context window can be taken from the batch size\n",
    "    \n",
    "    context_window = 2 * skip_window \n",
    "    # calculate the context_window...the total number of words around the target\n",
    "    # this gets rid of num_skips \n",
    "    num_labels = batch_size / context_window \n",
    "    # the number of labels is the how many context windows fit in the batch\n",
    "    # this changes the 1:2 relationship between target word and context words in skip-gram (\"predict context\")\n",
    "    # w/ a 2:1 relationship between context between context words and target words (\"predict word based on context\")\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(num_labels, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    for i in range(num_labels): # already divided num_labels replaces batch/num_skips\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        labels[i, 0] = buffer[target] # added this step to set labels outside context for loop\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(context_window): # now we're iterating thru context_window\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            # we've reversed target and batch below. further, we've removed step for creating labels\n",
    "            batch[i * context_window + j] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    return batch, labels\n",
    "\n",
    "skip_window = 1\n",
    "batch, labels = generate_batch_cbow(8, skip_window)\n",
    "print(np.shape(labels))\n",
    "for i in range(8):\n",
    "  print(batch[i], '->', labels[i/(2*skip_window), 0])\n",
    "  print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i/(2*skip_window), 0]])\n",
    "del skip_window # remove skip_window setting used for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Continuous-Bag-of-Words (CBOW) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11 12\n",
      " 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24\n",
      " 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33 34 34 35 35 36 36 37\n",
      " 37 38 38 39 39 40 40 41 41 42 42 43 43 44 44 45 45 46 46 47 47 48 48 49 49\n",
      " 50 50 51 51 52 52 53 53 54 54 55 55 56 56 57 57 58 58 59 59 60 60 61 61 62\n",
      " 62 63 63]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(xrange(valid_window), valid_size))\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "\n",
    "## General defines\n",
    "context_window = 2 * skip_window\n",
    "num_labels = batch_size / context_window\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[num_labels, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "  # seq_ids only needs to be generated once so do this as a numpy array rather than a tensor.\n",
    "  seq_ids = np.zeros(batch_size, dtype=np.int32)\n",
    "  cur_id = -1\n",
    "  for i in range(batch_size):\n",
    "    if i % context_window == 0:\n",
    "      cur_id = cur_id + 1\n",
    "    seq_ids[i] = cur_id\n",
    "  print(seq_ids)\n",
    "  \n",
    "  # use segment_sum to add together the related words and reduce the output to be num_labels in size.\n",
    "  final_embed = tf.segment_sum(embed, seq_ids)\n",
    "  \n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, final_embed,\n",
    "                               train_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 7.01279020309\n",
      "Nearest to its: automatism, hapless, outstanding, lewiston, d, ruiz, archetypical, barbie,\n",
      "Nearest to used: personally, surrey, kasner, preamble, ange, dunedin, aggregates, semper,\n",
      "Nearest to UNK: consume, scavengers, archiving, couple, pong, verses, snowfalls, chemicals,\n",
      "Nearest to this: barriers, disavow, winch, contracted, hamill, barrymore, squirrel, trintignant,\n",
      "Nearest to into: busts, enclosing, pleaded, moldavian, honour, biggest, satire, burgundians,\n",
      "Nearest to between: solving, supernaturalism, hughes, constricted, aap, ne, abitibi, cholesterol,\n",
      "Nearest to some: rterbuch, benzoyl, biome, js, majesty, wilmut, spore, womanhood,\n",
      "Nearest to use: noor, saban, airman, avery, body, unfashionable, rose, nicholls,\n",
      "Nearest to no: compacted, talkin, bfi, hasse, paulette, picardy, zeeland, obscene,\n",
      "Nearest to who: bainbridge, fortaleza, gorge, clements, chinaman, evaluated, boniface, drown,\n",
      "Nearest to that: ciboney, universelle, commentarii, circles, nobita, konami, splendid, halal,\n",
      "Nearest to zero: malabar, mithra, depleting, ystem, si, counterattacks, berkley, rack,\n",
      "Nearest to they: storybook, boucher, greyhounds, hendrick, praising, mouths, lica, hospital,\n",
      "Nearest to a: reformulation, structuralism, bennett, artistically, unforeseen, polemics, handwritten, ducati,\n",
      "Nearest to or: featureless, bearer, galloping, ccg, essences, schala, egbert, rothschild,\n",
      "Nearest to when: counterpoint, canidae, brahman, river, cebus, berliners, biome, abelian,\n",
      "Average loss at step 2000 : 3.69442095798\n",
      "Average loss at step 4000 : 3.03334254152\n",
      "Average loss at step 6000 : 3.04680948341\n",
      "Average loss at step 8000 : 2.9191692774\n",
      "Average loss at step 10000 : 2.78373635992\n",
      "Nearest to its: their, his, prototypical, tib, closed, her, exmouth, moor,\n",
      "Nearest to used: worded, claimed, aral, deepest, known, mendes, played, considered,\n",
      "Nearest to UNK: spotting, imported, vilas, tusks, cultivated, fresh, amaranthus, monolith,\n",
      "Nearest to this: it, that, the, no, celebrating, slums, villages, pheasants,\n",
      "Nearest to into: wallach, labial, cooperated, ttl, herr, gemstones, ers, donegal,\n",
      "Nearest to between: with, greenstone, vernet, actresses, solving, sparta, taipei, mirc,\n",
      "Nearest to some: many, these, any, ross, wysiwyg, dogmatically, beehives, regulations,\n",
      "Nearest to use: olives, gesner, way, archaically, yet, airman, affording, dahomey,\n",
      "Nearest to no: fugues, this, unpleasant, bouvier, konerko, generational, handwritten, criticizes,\n",
      "Nearest to who: he, they, also, zilog, jackass, hutch, fortaleza, which,\n",
      "Nearest to that: which, this, headquartered, woz, barreled, ashby, lovers, pollack,\n",
      "Nearest to zero: eight, nine, six, five, seven, three, four, moors,\n",
      "Nearest to they: he, there, we, who, digs, you, it, leningrad,\n",
      "Nearest to a: the, exclusively, omar, internationally, successful, kinship, honoring, deformations,\n",
      "Nearest to or: and, than, disappears, merciful, guan, sisterhood, but, whereas,\n",
      "Nearest to when: righteousness, where, helios, reshaping, bengals, before, myles, patronymic,\n",
      "Average loss at step 12000 : 2.81220418623\n",
      "Average loss at step 14000 : 2.76436321247\n",
      "Average loss at step 16000 : 2.69855254921\n",
      "Average loss at step 18000 : 2.71449082613\n",
      "Average loss at step 20000 : 2.69278956933\n",
      "Nearest to its: their, her, the, your, his, intelligence, prototypical, tib,\n",
      "Nearest to used: known, considered, found, given, published, written, shortwave, held,\n",
      "Nearest to UNK: arthur, bugtraq, eliminate, imperialist, pinch, ascribes, concealed, dd,\n",
      "Nearest to this: it, the, which, another, there, dopant, fadh, some,\n",
      "Nearest to into: on, in, from, quae, kraepelin, to, homestead, out,\n",
      "Nearest to between: with, actresses, headlined, capping, around, bishoprics, sunglasses, vernet,\n",
      "Nearest to some: many, several, these, all, both, certain, grumman, deflate,\n",
      "Nearest to use: way, baking, emissions, case, limbo, jezebel, veda, mattathias,\n",
      "Nearest to no: any, mansfield, fugues, bromwich, generational, precepts, hessian, generally,\n",
      "Nearest to who: he, never, always, they, which, not, often, already,\n",
      "Nearest to that: which, what, there, where, bloodlines, he, also, centrioles,\n",
      "Nearest to zero: five, eight, seven, four, six, nine, three, u,\n",
      "Nearest to they: we, he, there, you, i, she, not, decadent,\n",
      "Nearest to a: the, internationally, another, every, very, flute, honoring, haworth,\n",
      "Nearest to or: and, than, lymphoma, storm, kajang, irritated, tas, moreno,\n",
      "Nearest to when: if, racks, frantz, where, bengals, aeons, patronymic, booms,\n",
      "Average loss at step 22000 : 2.50478104235\n",
      "Average loss at step 24000 : 2.63834343451\n",
      "Average loss at step 26000 : 2.62632930547\n",
      "Average loss at step 28000 : 2.53193625921\n",
      "Average loss at step 30000 : 2.52840879318\n",
      "Nearest to its: their, her, your, his, intelligence, our, the, aerobic,\n",
      "Nearest to used: known, written, considered, found, available, called, referred, published,\n",
      "Nearest to UNK: koresh, rees, inning, davidson, arthur, motorola, dharmas, maja,\n",
      "Nearest to this: it, which, another, khanum, neustria, coexist, each, gaining,\n",
      "Nearest to into: through, from, ers, between, tentacle, off, up, within,\n",
      "Nearest to between: during, actresses, into, with, among, within, around, greenstone,\n",
      "Nearest to some: many, several, various, all, dreamland, deflate, these, most,\n",
      "Nearest to use: most, baking, concept, introduction, way, infinitum, motte, criminalized,\n",
      "Nearest to no: mansfield, little, surreal, prevail, any, than, considered, a,\n",
      "Nearest to who: also, he, still, actually, which, chaotic, lawful, nonconfirmative,\n",
      "Nearest to that: what, which, however, headquartered, wally, clav, retrospect, if,\n",
      "Nearest to zero: five, eight, seven, nine, two, three, six, four,\n",
      "Nearest to they: we, there, you, he, she, it, i, turbocharged,\n",
      "Nearest to a: any, another, the, teamwork, richness, epitaph, calcite, graduating,\n",
      "Nearest to or: than, and, but, woollen, realistic, dreaming, bats, invertible,\n",
      "Nearest to when: if, before, after, frantz, by, irreconcilable, until, where,\n",
      "Average loss at step 32000 : 2.51789132583\n",
      "Average loss at step 34000 : 2.46419091141\n",
      "Average loss at step 36000 : 2.09450875652\n",
      "Average loss at step 38000 : 2.08448540149\n",
      "Average loss at step 40000 : 2.1808048548\n",
      "Nearest to its: their, his, her, your, the, tib, surveillance, conic,\n",
      "Nearest to used: known, called, published, considered, seen, designed, referred, found,\n",
      "Nearest to UNK: h, graf, euboea, sasquatch, ludwig, suborder, wyatt, billet,\n",
      "Nearest to this: another, which, some, the, it, bryant, each, our,\n",
      "Nearest to into: through, dez, from, up, restated, out, between, off,\n",
      "Nearest to between: with, greenstone, among, solving, within, into, during, afghani,\n",
      "Nearest to some: many, several, these, all, certain, deflate, this, dreamland,\n",
      "Nearest to use: baking, mathbf, hymenoptera, failure, kodansha, concept, area, impoverishment,\n",
      "Nearest to no: any, little, mansfield, unpleasant, hyi, coexistence, prevail, afghana,\n",
      "Nearest to who: he, often, they, sometimes, still, never, actually, abernethy,\n",
      "Nearest to that: which, however, palpatine, what, scoreless, visits, clav, tangerine,\n",
      "Nearest to zero: four, eight, five, seven, six, nine, three, two,\n",
      "Nearest to they: he, there, we, she, you, who, never, lawful,\n",
      "Nearest to a: the, too, another, honoring, handwritten, haworth, this, interconnected,\n",
      "Nearest to or: and, than, sylheti, jitsu, but, inflammatory, dst, like,\n",
      "Nearest to when: after, before, if, during, where, although, though, until,\n",
      "Average loss at step 42000 : 2.3323714731\n",
      "Average loss at step 44000 : 2.34919334795\n",
      "Average loss at step 46000 : 2.38907744703\n",
      "Average loss at step 48000 : 2.3167549392\n",
      "Average loss at step 50000 : 2.39958460953\n",
      "Nearest to its: their, his, the, her, tib, our, your, averaging,\n",
      "Nearest to used: published, known, seen, observed, referred, regarded, found, described,\n",
      "Nearest to UNK: ch, theodora, katsuhiro, vanadium, mandeville, hercules, complemented, suffix,\n",
      "Nearest to this: the, which, another, khanum, disavowed, bryant, our, it,\n",
      "Nearest to into: through, from, across, off, up, detailing, ers, dez,\n",
      "Nearest to between: with, among, within, greenstone, betrothal, into, tomato, solving,\n",
      "Nearest to some: many, several, any, all, each, most, those, dreamland,\n",
      "Nearest to use: support, hymenoptera, mathbf, exception, negating, llewellyn, baking, secrete,\n",
      "Nearest to no: little, any, prevail, hessian, another, fugue, mansfield, newline,\n",
      "Nearest to who: which, often, actually, they, woodside, she, generally, he,\n",
      "Nearest to that: which, how, what, delivers, headquartered, where, regretted, who,\n",
      "Nearest to zero: six, seven, eight, five, four, nine, three, children,\n",
      "Nearest to they: we, there, he, she, you, who, it, lawful,\n",
      "Nearest to a: another, every, calcite, the, artistically, deformations, tricolor, handwritten,\n",
      "Nearest to or: and, than, omega, glories, kajang, cub, thomas, pacemakers,\n",
      "Nearest to when: partido, where, though, after, if, irreconcilable, plo, before,\n",
      "Average loss at step 52000 : 2.43825442579\n",
      "Average loss at step 54000 : 2.47621179521\n",
      "Average loss at step 56000 : 2.38759249833\n",
      "Average loss at step 58000 : 2.43236331102\n",
      "Average loss at step 60000 : 2.13443680275\n",
      "Nearest to its: their, his, exmouth, her, our, defy, your, the,\n",
      "Nearest to used: seen, known, found, defined, represented, described, referred, regarded,\n",
      "Nearest to UNK: biotechnology, stuart, mart, hugo, dr, r, en, www,\n",
      "Nearest to this: which, it, any, registries, criswell, sutter, cumbria, fidelity,\n",
      "Nearest to into: through, across, periodic, throughout, from, narration, inside, dez,\n",
      "Nearest to between: among, with, within, throughout, in, around, across, jati,\n",
      "Nearest to some: many, several, all, certain, any, these, different, those,\n",
      "Nearest to use: form, share, support, minyan, hymenoptera, play, natively, jonah,\n",
      "Nearest to no: little, any, another, neodymium, always, or, prevail, wahhabi,\n",
      "Nearest to who: they, actually, never, often, he, bets, woodbridge, still,\n",
      "Nearest to that: which, however, where, woz, forrester, what, bloodlines, regretted,\n",
      "Nearest to zero: eight, five, four, seven, nine, six, three, two,\n",
      "Nearest to they: we, she, he, there, you, who, it, i,\n",
      "Nearest to a: another, sufficiently, concourse, richness, jmp, bourbaki, aliphatic, herero,\n",
      "Nearest to or: and, than, internationale, no, cub, jodie, psychosomatic, indomitable,\n",
      "Nearest to when: if, before, where, though, although, mundo, following, auc,\n",
      "Average loss at step 62000 : 2.35859282535\n",
      "Average loss at step 64000 : 2.39802624926\n",
      "Average loss at step 66000 : 2.3569876307\n",
      "Average loss at step 68000 : 2.39154952388\n",
      "Average loss at step 70000 : 2.41735287589\n",
      "Nearest to its: their, his, whose, her, the, our, your, defy,\n",
      "Nearest to used: represented, known, found, described, referred, designed, seen, considered,\n",
      "Nearest to UNK: homicide, olds, der, dactylic, clemens, emirate, awami, hardy,\n",
      "Nearest to this: which, the, another, pheasants, any, some, his, what,\n",
      "Nearest to into: through, from, down, onto, diophantus, under, tones, raisonn,\n",
      "Nearest to between: with, among, within, in, around, cretian, using, from,\n",
      "Nearest to some: many, several, certain, these, various, any, each, dreamland,\n",
      "Nearest to use: support, rydberg, share, form, cliff, think, neutrino, sell,\n",
      "Nearest to no: only, any, little, another, anticonvulsants, caiman, nullification, wahhabi,\n",
      "Nearest to who: he, actually, never, generally, often, typically, still, hutch,\n",
      "Nearest to that: which, however, drowning, headquartered, once, when, what, ritually,\n",
      "Nearest to zero: five, seven, six, eight, nine, four, three, two,\n",
      "Nearest to they: he, she, there, we, you, i, eventually, later,\n",
      "Nearest to a: the, another, any, artistically, mckenna, richness, calcite, herero,\n",
      "Nearest to or: than, and, strathclyde, porky, aridity, pedro, inflammatory, ecozone,\n",
      "Nearest to when: though, while, before, although, if, until, after, because,\n",
      "Average loss at step 72000 : 2.35255009824\n",
      "Average loss at step 74000 : 2.3935705601\n",
      "Average loss at step 76000 : 2.08665111928\n",
      "Average loss at step 78000 : 2.0159537472\n",
      "Average loss at step 80000 : 2.26949592564\n",
      "Nearest to its: their, whose, his, your, the, her, our, exmouth,\n",
      "Nearest to used: seen, required, found, described, referred, known, represented, considered,\n",
      "Nearest to UNK: i, opined, atheromatous, pe, bradshaw, frankfort, disposed, mart,\n",
      "Nearest to this: which, another, the, augusti, it, pda, particular, registries,\n",
      "Nearest to into: onto, through, back, from, down, out, heavier, via,\n",
      "Nearest to between: with, among, within, in, throughout, solving, during, cretian,\n",
      "Nearest to some: many, several, these, various, critics, numerous, societal, both,\n",
      "Nearest to use: support, contribution, call, popularity, exception, form, gesner, infinitum,\n",
      "Nearest to no: little, complete, jebel, prevail, roux, massed, varepsilon, pompous,\n",
      "Nearest to who: often, never, which, he, actually, generally, traditionally, still,\n",
      "Nearest to that: which, what, however, losing, delusional, when, clav, scoreless,\n",
      "Nearest to zero: eight, six, five, seven, four, three, nine, two,\n",
      "Nearest to they: we, there, he, you, she, it, lawful, injure,\n",
      "Nearest to a: another, any, herero, emulate, calcite, artistically, handwritten, honeybee,\n",
      "Nearest to or: and, than, though, diverting, changeover, boyce, cassettes, but,\n",
      "Nearest to when: if, before, while, though, although, after, during, partido,\n",
      "Average loss at step 82000 : 2.28354332578\n",
      "Average loss at step 84000 : 2.21031179655\n",
      "Average loss at step 86000 : 2.27771613955\n",
      "Average loss at step 88000 : 2.35161925387\n",
      "Average loss at step 90000 : 2.22836437818\n",
      "Nearest to its: their, his, your, the, her, our, whose, exmouth,\n",
      "Nearest to used: known, seen, done, referred, written, available, held, represented,\n",
      "Nearest to UNK: parker, stuart, pianist, mohammed, clemens, pneumonic, tasked, boris,\n",
      "Nearest to this: which, it, what, pheasants, that, some, honeydew, the,\n",
      "Nearest to into: through, onto, down, from, off, across, back, within,\n",
      "Nearest to between: within, among, with, throughout, solving, into, sunglasses, mired,\n",
      "Nearest to some: many, several, numerous, these, certain, various, carbonated, their,\n",
      "Nearest to use: support, form, popularity, consist, contribution, start, bibliographic, care,\n",
      "Nearest to no: little, headache, excellent, laterally, diatoms, cx, gallurese, lindh,\n",
      "Nearest to who: which, never, also, he, correctly, they, woodside, she,\n",
      "Nearest to that: which, what, woz, where, malpighi, this, latter, mindstorms,\n",
      "Nearest to zero: six, seven, five, eight, nine, three, four, folketing,\n",
      "Nearest to they: we, he, she, you, there, it, soon, who,\n",
      "Nearest to a: another, ucr, richness, the, artistically, tricolor, deceptive, benny,\n",
      "Nearest to or: than, and, while, whereas, inflammatory, pedro, dhyana, siwa,\n",
      "Nearest to when: while, if, before, though, after, where, although, bengals,\n",
      "Average loss at step 92000 : 2.25446543363\n",
      "Average loss at step 94000 : 2.28704337599\n",
      "Average loss at step 96000 : 2.27545172293\n",
      "Average loss at step 98000 : 2.38467882863\n",
      "Average loss at step 100000 : 2.24091686257\n",
      "Nearest to its: their, his, our, the, her, whose, your, eyelids,\n",
      "Nearest to used: described, referred, seen, done, created, found, held, required,\n",
      "Nearest to UNK: malone, samson, aoc, swapped, mart, clan, retroflex, compensating,\n",
      "Nearest to this: which, it, pheasants, aristotle, there, that, itself, criswell,\n",
      "Nearest to into: onto, through, from, within, down, blore, across, under,\n",
      "Nearest to between: with, among, within, amongst, hagar, sunglasses, solving, brigitte,\n",
      "Nearest to some: many, several, these, numerous, most, any, dreamland, all,\n",
      "Nearest to use: support, way, exception, form, contribution, usage, consist, study,\n",
      "Nearest to no: any, little, a, headache, sepia, radbruch, spitzer, neither,\n",
      "Nearest to who: still, often, actually, she, he, never, generally, typically,\n",
      "Nearest to that: which, what, however, clav, how, syringe, drowning, this,\n",
      "Nearest to zero: five, eight, six, nine, seven, four, three, million,\n",
      "Nearest to they: we, there, he, you, she, i, it, who,\n",
      "Nearest to a: another, no, the, isosceles, herero, artistically, cowpox, every,\n",
      "Nearest to or: and, inexplicable, biennial, rectangular, commentarii, tetrachloride, spivak, susie,\n",
      "Nearest to when: though, if, although, after, before, while, during, until,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_data, batch_labels = generate_batch_cbow(batch_size, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step\", step, \":\", average_loss)\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = \"Nearest to %s:\" % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = \"%s %s,\" % (log, close_word)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs & Results\n",
    "\n",
    "1. Skip-gram, skip_window = 1, num_skips = 2, batch_size = 128, steps = 100k, error: 3.359206\n",
    "2. skip_window = 2, num_skips = 4, error: 3.224013\n",
    "3. CBOW, skip_window = 1, batch = 128, steps = 100k, error: 2.24091686257\n",
    "    a. 2k: Nearest to between: solving, supernaturalism, hughes, constricted, aap, ne, abitibi, cholesterol,\n",
    "    b. 100k: Nearest to between: with, among, within, amongst, hagar, sunglasses, solving, brigitte,\n",
    "4. CBOW, skip_window = 2, error: 3.16\n",
    "    a. 2K: Nearest to history: difficulties, serapis, verify, macrolides, abyss, uplifting, stipend, united,\n",
    "    b. 100k: Nearest to history: sotho, supports, generalissimo, propped, variational, action, articles, sustainability,\n",
    "CONCLUSION - CBOW WORKS BETTER, PARTICULARLY WHEN NOISE IS KEPT AT A MINIMUM I.E., PREDICTED BY TWO WORDS CLOSEST."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "5_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
