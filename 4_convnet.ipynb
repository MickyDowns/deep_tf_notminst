{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key to this section is understanding how tf.nn.conv2d computes a 2-D convolution given 4-D input and filter tensors. \n",
    "\n",
    "tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)\n",
    "\n",
    "Args:\n",
    "input: A Tensor. Must be one of the following types: float32, float64.\n",
    "filter: A Tensor. Must have the same type as input.\n",
    "strides: A list of ints. 1-D of length 4. The stride of the sliding window for each dimension of input.\n",
    "padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "use_cudnn_on_gpu: An optional bool. Defaults to True.\n",
    "name: A name for the operation (optional).\n",
    "\n",
    "So, it takes:\n",
    "1. an input tensor (\"input\" above) of shape [batch, in_height, in_width, in_channels]\n",
    "    + this is variously data (to start), hidden (before pooling added) or pool\n",
    "2. a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    + these are the weights\n",
    "    + it expects the layout to be: [filter_height, filter_width, in_channels, out_channels]\n",
    "    + this is why we're seeing weights initialized as [patch_size, patch_size, num_channels, depth]\n",
    "    + ON EACH ITERATION THE FILTER IS ESSENTIALLY THE TARGET YOU WANT TO TRANSFORM TO.\n",
    "3. strides\n",
    "    + [1,2,2,1] w/out pooling, [1,1,1,] after pooling\n",
    "    + presumably w/out strides > 1 (or pre-pooling), you're re-shaping the data, but not reducing it. \n",
    "4. padding = \"SAME\"\n",
    "    \n",
    "It then: \n",
    "1. Flattens the FILTER to a 2-D matrix with shape [filter_height * filter_width * in_channels, output_channels].\n",
    "    + so, it takes the filter matri and flattens it out. \n",
    "2. Extracts image patches from the input tensor to form a virtual tensor of shape:\n",
    "        [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "    + so, note that both have filter_height * filter_width * in_channel components\n",
    "3. For each patch, right-multiplies the filter matrix and the image patch vector.\n",
    "\n",
    "It does this by:\n",
    "output[b, i, j, k] =\n",
    "    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] * # reducing the image\n",
    "                    filter[di, dj, q, k] # applying the filter (weights) through matrix multiplication.\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "\n",
    "batch_size = 64 # was 16\n",
    "\n",
    "# convolutional layers\n",
    "patch_size = 10 # was 5, picked up 1% w/ 10\n",
    "depth = 16 # how was this determined? not related to batch size?\n",
    "\n",
    "# connected layers\n",
    "con1_num_hidden = 128\n",
    "con2_num_hidden = 64 # how was this determined?\n",
    "\n",
    "# dropout\n",
    "dropout = False\n",
    "position1_keep_prob = 0.7\n",
    "position2_keep_prob = 0.6\n",
    "\n",
    "# learning\n",
    "learning = True\n",
    "start_learn = 0.1\n",
    "learn_decay = 0.9\n",
    "learn_step = 200\n",
    "stair = True\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables. building 4D tensors w/ dims: [batch, height, width, channels] or [b, y, x, c]\n",
    "    ## layer 1: convolutional. the is the target structure. 2d patchXpatch, but w/ much greater depth.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth])) \n",
    "    ## biases initialized w/ \"depth\" dimension\n",
    "    \n",
    "    ## layer 2: conoluational\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    ## layer 3: connected net that is 1. flattended (2d) and 2. implies stride = 4 or 2*(2x) downsampling\n",
    "    in_size = image_size // 4 * image_size // 4 * depth\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([in_size, con1_num_hidden], stddev = math.sqrt(2.0/in_size)))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[con1_num_hidden]))\n",
    "    \n",
    "    ## layer 4: fully connected\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([con1_num_hidden,con2_num_hidden],\n",
    "                                                     stddev = math.sqrt(2.0/con1_num_hidden)))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[con2_num_hidden]))\n",
    "    \n",
    "    ## layer 5 or \"output\" layer maps from hidden nodes (64) to output nodes (10)\n",
    "    layer5_weights = tf.Variable(tf.truncated_normal([con2_num_hidden, num_labels], \n",
    "                                                     stddev = math.sqrt(2.0/con2_num_hidden)))\n",
    "    layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data): # interesting that no inputs for regularization, dropout\n",
    "        \n",
    "        # could try pooling here.\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        # telling it to build convnet taking input data record, mapping it to layer1.\n",
    "        ## vector of dims seems to be communicating stride for x and y axis in center of 4d object.\n",
    "        hidden = tf.nn.relu(conv + layer1_biases) ## conv was output weights to which you add biaes and run relu.\n",
    "        \n",
    "        pool = tf.nn.max_pool(hidden, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")     \n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        \n",
    "        pool = tf.nn.max_pool(hidden, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\") \n",
    "        shape = pool.get_shape().as_list() # was hidden\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]]) # collapsing dims 1-3 (sparse result?)\n",
    "        \n",
    "        if dropout:\n",
    "            reshape = tf.nn.dropout(reshape,position1_keep_prob)\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        \n",
    "        if dropout:\n",
    "            hidden = tf.nn.dropout(hidden,position2_keep_prob)\n",
    "        hidden = tf.nn.relu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n",
    "        \n",
    "        return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    ## insert regularization here\n",
    "    \n",
    "    # Learning and Optimization\n",
    "    if learning: \n",
    "        global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "        learning_rate = tf.train.exponential_decay(start_learn, global_step, learn_step, learn_decay, staircase=stair)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    else:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "    ## inception:\n",
    "    ## http://arxiv.org/pdf/1409.4842v1.pdf\n",
    "    ## https://www.tensorflow.org/versions/0.6.0/tutorials/image_recognition/index.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 5.699503\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 250: 0.565993\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 500: 0.808567\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 750: 0.662197\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 1000: 0.500157\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1250: 0.317431\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1500: 0.639241\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1750: 0.529561\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2000: 0.187477\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2250: 0.506581\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2500: 0.329775\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2750: 0.473544\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3000: 0.290320\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 3250: 0.424801\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 3500: 0.382342\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3750: 0.450155\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 4000: 0.250582\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Test accuracy: 94.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :] # one more dimension. 4 total\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 250 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. baseline conv2 w/ stride 2: 88.x%\n",
    "2. pre-process 1st conv w/ max pool of stride = 2 and kernel = 2: 89%\n",
    "3. avg_pool: 89.4%\n",
    "4. boosted patch size to 10 from 5: 89.7%\n",
    "5. re-configured pool. now two pools after each convolution: 89.8%\n",
    "6. three pools: 90.2%\n",
    "7. dropout position 1, 0.5 keep prob: 86.7%\n",
    "8. dropout at position 2, 0.5 keep prob: 80.4% - CONCLUSION: retry dropout in position1 when you get to more records.\n",
    "9. learning (0.25, 0.75, 100, True): 10%\n",
    "    100 0.25 * 0.75^(100/100) = 0.188\n",
    "    ...\n",
    "    1000 0.25 * 0.75^(1000/100) = 0.014\n",
    "    CONCLUSION: learning unstable. retry more gradual and/or boost steps.  \n",
    "10. learning (0.1, 0.95, 200, True): 89%\n",
    "    100 0.1 * 0.95^(200/200) = 0.095\n",
    "    ...\n",
    "    2000 0.1 * 0.95^(1000/200) = 0.077\n",
    "11. doubled steps and smoothed learning: 91.8%\n",
    "    100 0.1 * 0.90^(200/200) = 0.095\n",
    "    ...\n",
    "    1000 0.1 * 0.90^(2000/200) = 0.034\n",
    "12. back to dropout, position 1, 0.7 keep prob. 92.2% - CONCLUSION: dropout is additive when you get to appropriate scale. key here is that validation set still improving at 2000 steps indicating that we're not overfitting. \n",
    "13. get baseline for dropout / learning before testing second dropout. 3000 steps, 64 batch size: 94.5% \n",
    "    NOTE: it's still underfitting.  \n",
    "14. try w/ dropout position1 and position2: 93.3% - CONCLUSION: dropout before return hurts performance. likely b/c there's no opportunity for model to adjust to missing readings before it's input to next convolution w/ new data. \n",
    "15. adding 2nd fully connected layer. so, first layer 64->128, second layer 64: 94.7%\n",
    "16. misc: 93.9%\n",
    "    a. initialize using 2/N thing\n",
    "    b. declining keep probs\n",
    "    c. layer size: layer 1 128->320 (10*16*2):\n",
    "    \"net architectures usually sharpen towards the output layer so you force them to learn the main classification features, they are pyramid like, the more so for convolutional layers. The rest of the parameters was chosen by trial and error, choose whatever works better on the validation set.\"\n",
    "    \n",
    "17. backing up: layer size back to 128. 94.8%\n",
    "18. patch back to 5: 93.8\n",
    "19. 2nd convolution from 16 to 32?\n",
    "20. avg vs. max: 94.5%\n",
    "21. pooling 3 or 4 vs 2?\n",
    "22. 4000 steps\n",
    "23. inception\n",
    "24. using endri.deliu's model achieved 95.3% on 5,0001 steps.\n",
    "\n",
    "\n",
    "Optimization dimensions:\n",
    "1. conv layers\n",
    "    a. number\n",
    "    b. size\n",
    "    c. max/ave stride\n",
    "    d. pooling\n",
    "    e. \n",
    "2. nnet\n",
    "    a. layers\n",
    "    b. learning\n",
    "    c. dropout\n",
    "3. other\n",
    "    a. batch size\n",
    "    b. steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@endri.deliu's ELU code. One thing I did play with was the elu neurons (as opposed to relu). ELU-s use an exponential function similar in shape to RELU-s but they can have negative values and really start shining in deeper nets. Using elus on my deep net of 5 conv layers and 2 fully connected layers made the model converge to the final result much sooner (about 2-3x faster) and I was able to achieve 93.4% on the validation set and 97.8% on the test set. Elus make sense if you have a lot of convolutions and/or fully connected layers and are also cheaper computationally. My previous conv net was using batch normalization and got me about 97.7%. Using ELU-s did speed things up significantly. The ELU paper: http://arxiv.org/abs/1511.0728912. If anyone is interested I can post the code of my conv net. I am pretty sure with further tweaking of the hyperparameters you can get even better accuracy. Again, had to use just 12k images (out of 18k) to calculate the test set error, otherwise my computer would throw an out of memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 2, 2, 256]\n",
      "[10000, 2, 2, 256]\n",
      "[10000, 2, 2, 256]\n"
     ]
    }
   ],
   "source": [
    "#Below the code using ELU-s. The batch normalization steps are commented out.\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 3\n",
    "depth = 16\n",
    "num_hidden = 705\n",
    "num_hidden_last = 205\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables. 5 convolutional and 3 fully connected layers. # conv: 4D tensors [batch, height, width, channels]\n",
    "    layerconv1_weights = tf.Variable(tf.truncated_normal( # batch = 16, patch = 3, channels = 1, depth = 16 \n",
    "            [patch_size, patch_size, num_channels, depth], stddev=0.1)) \n",
    "            # so, this is batch of 3, 3x1's, w/ depth 16 i.e., take each 3x1 and map it to 3?x16. \n",
    "    layerconv1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    # so, one key to his implementation is num_channels -> depth -> 2depth -> 4depth ->4depth, \n",
    "    # but then ending on 16depth. so, really focusing the information in each pixel into new dimension\n",
    "    layerconv2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth, depth * 2], stddev=0.1)) # so, this is how me modified depth in layer 2\n",
    "            # only thing that changes so, just forcing data into longer tube\n",
    "    layerconv2_biases = tf.Variable(tf.zeros([depth * 2]))\n",
    "    \n",
    "    layerconv3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth * 2, depth * 4], stddev=0.03)) # doubling depth again\n",
    "    layerconv3_biases = tf.Variable(tf.zeros([depth * 4]))\n",
    "    \n",
    "    layerconv4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth * 4, depth * 4], stddev=0.03)) # stable depth\n",
    "    layerconv4_biases = tf.Variable(tf.zeros([depth * 4]))\n",
    "    \n",
    "    layerconv5_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth * 4, depth * 16], stddev=0.03)) # 4x depth jump. 246 layers\n",
    "    layerconv5_biases = tf.Variable(tf.zeros([depth * 16]))\n",
    "\n",
    "    # the other is knowing what size to make the convolutional layer given later pooling.\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal( # first fully connected layer\n",
    "            [image_size / 7 * image_size / 7 * (depth * 4), num_hidden], stddev=0.03))\n",
    "            # the shape of each input (image) has changed from 784 to 1024 input nodes. \n",
    "    layer3_biases = tf.Variable(tf.zeros([num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden, num_hidden_last], stddev=0.0532))\n",
    "    layer4_biases = tf.Variable(tf.zeros([num_hidden_last]))\n",
    "    \n",
    "    layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden_last, num_labels], stddev=0.1))\n",
    "    layer5_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data, use_dropout=False):\n",
    "        # no initial pool here.\n",
    "        conv = tf.nn.conv2d(data, layerconv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.elu(conv + layerconv1_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, layerconv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.elu(conv + layerconv2_biases)\n",
    "        #pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, layerconv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.elu(conv + layerconv3_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        # norm1: Normalization is useful to prevent neurons from saturating when inputs may have \n",
    "        # varying scale, and to aid generalization.\n",
    "        # norm1 = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, layerconv4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.elu(conv + layerconv4_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        # norm1 = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, layerconv5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.elu(conv + layerconv5_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        # norm1 = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "        \n",
    "        shape = pool.get_shape().as_list()\n",
    "        print(shape)\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.elu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        \n",
    "        if use_dropout:\n",
    "            hidden = tf.nn.dropout(hidden, 0.75)\n",
    "            \n",
    "        nn_hidden_layer = tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "        hidden = tf.nn.elu(nn_hidden_layer)\n",
    "        \n",
    "        if use_dropout:\n",
    "            hidden = tf.nn.dropout(hidden, 0.75)\n",
    "            \n",
    "        return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 3000, 0.86, staircase=True)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 2.31148\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 500 : 0.504193\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 1000 : 0.742254\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1500 : 0.704914\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2000 : 0.746453\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2500 : 0.370096\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 3000 : 0.458386\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3500 : 0.255316\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 4000 : 0.257595\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4500 : 0.286413\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 5000 : 0.675464\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.0%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001 # was 95001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in xrange(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step\", step, \":\", l)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            #print(time.ctime())\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# End nn used two convolution layers, then the inception module, then three fully connected layers. \n",
    "# Result was 96.8% on the test set. Happy to hear if this is set up correct, thanks.\n",
    "# https://discussions.udacity.com/t/assignment-4-problem-2/46525/41\n",
    "\n",
    "def inception_layer1(data):\n",
    "    # Inception 1x1\n",
    "    conv_1x1 = tf.nn.conv2d(data, inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv_1x1 = tf.nn.relu(conv_1x1 + inception_1x1_biases)\n",
    "    ## 1x1 - before the bigger patches\n",
    "    conv_pre = tf.nn.conv2d(data, pre_inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv_pre = tf.nn.relu(conv_pre + pre_inception_1x1_biases)\n",
    "    # Pooling 3x3\n",
    "    ## average pool followed by a 1x1\n",
    "    conv_pool = tf.nn.avg_pool(data, [1, 3, 3, 1], [1, 1, 1, 1], padding='SAME')\n",
    "    conv_pool = tf.nn.conv2d(conv_pool, inception_1x1_pool_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv_pool = tf.nn.relu(conv_pool + inception_1x1_pool_biases)\n",
    "    # Inception 3x3\n",
    "    ## 1x1 followed by a 3x3 (i actually read it in his voice)\n",
    "    conv_3x3 = tf.nn.conv2d(conv_pre, inception_3x3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv_3x3 = tf.nn.relu(conv_3x3 + inception_3x3_biases)\n",
    "    # Inception 5x5\n",
    "    ## 1x1 followed by a 5x5\n",
    "    conv_5x5 = tf.nn.conv2d(conv_pre, inception_5x5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv_5x5 = tf.nn.relu(conv_5x5 + inception_5x5_biases)\n",
    "    return tf.concat(3, [conv_1x1, conv_3x3, conv_5x5, conv_pool])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
