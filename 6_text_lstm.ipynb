{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "# Base: Utility functions map characters and vocabulary IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0]) # brings back integer value for unicode char \n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase: # these are letters a to z\n",
    "        # so, if the input character is in a->z, return its integer representation\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, this shows us that char2id returns an integer that corresponds to the unicode letter (character) when known and 0 when blank or not known. it also shows the reverse.\n",
    "\n",
    "ord(c)\n",
    "Given a string of length one, return an integer representing the Unicode code point of the character when the argument is a unicode object, or the value of the byte when the argument is an 8-bit string. For example, ord('a') returns the integer 97, ord(u'\\u2020') returns 8224. This is the inverse of chr() for 8-bit strings and of unichr() for unicode objects. If a unicode argument is given and Python was built with UCS2 Unicode, then the character’s code point must be in the range [0..65535] inclusive; otherwise the string length is two, and a TypeError will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz\n",
      "27\n",
      "100000000\n",
      "1562500\n"
     ]
    }
   ],
   "source": [
    "print(string.ascii_lowercase)\n",
    "print(vocabulary_size)\n",
    "print(len(text))\n",
    "print(len(text)// batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "# base: (training) batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64 # this is the number of text characters in the input mini-batch\n",
    "num_unrollings=10\n",
    "\n",
    "# building a BatchGenerator class of objects with methods to:\n",
    "# 1. initialize a set of variables\n",
    "# 2. generate batches of size = 64 of 11 letter segments\n",
    "# 3. \n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text # this is loading up the 100M character text file\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size # 1.6M batches\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        # tracks cursor position, incrementing its position by 64 places \n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self): \n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        # initializing a matrix of batch size = 64 X vocabulary size = 27.\n",
    "        for b in range(self._batch_size): # for 0 -> 63\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0 \n",
    "            # put a 1 in column corresponding to the integer id of the text character the cursor is on\n",
    "            # do that for each character in the current batch\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size # increment the cursor\n",
    "        return batch\n",
    "    # so, result here s/b a sparse matrix w/ 64 rows, one for each input character, and\n",
    "    # 27 rows, one for each letter of the alphabet. mostly 0's except in column corresponding\n",
    "    # to ord integer value for each input character.\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\"\"\"\n",
    "        batches = [self._last_batch] # set batch = to the last batch in prior set\n",
    "        for step in range(self._num_unrollings): # 0->9+1 or 11\n",
    "            batches.append(self._next_batch()) # append the next sequence of 10 batches\n",
    "        self._last_batch = batches[-1] # set last batch pointer to the 10th position\n",
    "        return batches\n",
    "    # so, result here s/b batches which is ten 64 x 27 matrices appended vertically. \n",
    "    # however, output below seems like it's 64 1x10 arrays. \n",
    "    \n",
    "    def characters(probabilities):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) character representation.\"\"\"\n",
    "        return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "    # so, take in one-hot encoding and return its actual character representation\n",
    "    \n",
    "    def batches2string(batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string representation.\"\"\"\n",
    "        s = [''] * batches[0].shape[0]\n",
    "        for b in batches:\n",
    "            s = [''.join(x) for x in zip(s, characters(b))] \n",
    "            # join concatenates (end to beginning) strings\n",
    "            # zip zips the strings together forming a tuple\n",
    "        return s\n",
    "    # example output below\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "\n",
    "# so, below is what two training batch looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip([iterable, ...]), as in \"like a zipper for joining tubles\"\n",
    "This function returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The returned list is truncated in length to the length of the shortest argument sequence. When there are multiple arguments which are all of the same length, zip() is similar to map() with an initial argument of None. With a single sequence argument, it returns a list of 1-tuples. With no arguments, it returns an empty list.\n",
    "\n",
    "The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n). zip() in conjunction with the * operator can be used to unzip a list:\n",
    ">>>\n",
    ">>> x = [1, 2, 3]\n",
    ">>> y = [4, 5, 6]\n",
    ">>> zipped = zip(x, y)\n",
    ">>> zipped\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    ">>> x2, y2 = zip(*zipped)\n",
    ">>> x == list(x2) and y == list(y2)\n",
    "True\n",
    "\n",
    "***\n",
    "\n",
    "7.1.5. String functions\n",
    "The following functions are available to operate on string and Unicode objects. They are not available as string methods.\n",
    "\n",
    "string.join(words[, sep])\n",
    "Concatenate a list or tuple of words with intervening occurrences of sep. The default value for sep is a single space character. It is always true that string.join(string.split(s, sep), sep) equals s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base: sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "    # assuming labels are one-hot, this seems to be capturing predicted probability of the single \"1\" label\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "    r = random.uniform(0, 1) # generates single num btwn 0 and 1\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "    # so, while later it will be good to save multiple sequences selecting the best,\n",
    "    # here we're generating a random number r (the \"hurdle\", then selecting the value (observation?) \n",
    "    # from the input distribution that \"tips the probability scale\" over the r \"hurdle\"\n",
    "    # I think predictions include predicted values and some sample of non-predicted or 0 values.\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "    # so, this is taking that tipping point from above and one-hot encoding the position on the prediction vector\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922659605717\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(random.uniform(0, 1))\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "# base: define simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "        # these are 27 x 64 matrices\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.                          \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf \n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib) \n",
    "        # applying logistic to sum of input + previous output + bias for input gate\n",
    "        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # applying logistic to sum of input + previous output + bias\n",
    "        \n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        # update dependent on input, state and bias of the memory cell (\"c\")\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        # state multiplies the decimal values of the forget logistic * (prior?) state + input logistic * tanh\n",
    "        \n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # output gate is sigmoid of input, output and bias to output gate\n",
    "        \n",
    "        # output sigmoid is function always of input and prior state, but also some above threshold memory\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        # batch_size = 64, vocabulary_size = 27\n",
    "        # so, for each \"unrolling\" we're appending 1608 length vector to train_data \n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings] # so, each input is 11 characters. there s/b 64 per batch\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. need to see how used.\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list() # blank list object\n",
    "    output = saved_output # s_o saves state across unrollings\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state) \n",
    "        # pass lstm function the training input record, the prior output and the prior state. get update\n",
    "        outputs.append(output) # add output to the stack\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # looks like it's multiplying input (x) * weights (w) + biases (b) to get logits\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "        # running softmax cross entropy loss function using logits, train_labels\n",
    "        \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        # decaying (stepping way down (stair=True)) learning rate every 5k steps i.e., from 10 to 1\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        # passing loss function, computing gradient on loss, unzipping to provide gradients and \"v\"\n",
    "    \n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        # clipping to eliminate possible exploding gradient\n",
    "        \n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "        # re-zipping possibly corrected gradient and \"v\", applying those gradients to optimizer\n",
    "        \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "        # simple softmax of previously calculated logits\n",
    "        \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        # using the last sample output and state, softmax predict on logits from current output AND current w & b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base: run simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298881 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "kkm aananbohnxnpg ia  evg etleis oftxr yez ibfr eans r vqecr evhvztgsuv dm  teih\n",
      "vojd tdbix flpngneofg dhrreclsg rm  yexmsygawaktvvq b onvkik  zrtr  hu pidoaa vs\n",
      "gw ieklknjcto xr nsnn wcpy  tioepenujong ww fs ibeqtkdwp tpbnbvpntluojwuiwytabfn\n",
      "kmiahmst ani  ctqinf ahmag  emylk ioth  oumvb jdwhyinpmfb rnoayxhtm  ytl tyttbae\n",
      "e bs y mlpttrbdy  vt etyisntqnguycrcnrjrlxdflnfcoyc tfgnoqp ehfhbnswgpmdh pmonca\n",
      "================================================================================\n",
      "Validation set perplexity: 20.00\n",
      "Average loss at step 100: 2.595968 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.86\n",
      "Validation set perplexity: 10.24\n",
      "Average loss at step 200: 2.255202 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 300: 2.082265 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 1.991680 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 500: 1.995071 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 600: 1.919294 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 700: 1.890212 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 800: 1.869269 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 900: 1.854960 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 1000: 1.789391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "war podsicar the gortoke huchical that as hooly in jonth worr the jary than the \n",
      "mbly one nine one nine six ruse was be stlaidina heast cinculd stice of hardest \n",
      " decage sc exuted theye nor unice poutl e ombges tap heportems of lagen gave lac\n",
      "x weep gear goot and k to mork has conor jebmhest the redue beingan copmonces as\n",
      "ver opernay the could zero nits amcion the eal hard pop te as unfrous the play t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1100: 1.765587 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1200: 1.791913 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1300: 1.776742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.745799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1500: 1.736982 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1600: 1.722296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1700: 1.743979 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1800: 1.710302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1900: 1.712618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2000: 1.720911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "d con one eight amerial workw day is and the feal spacting of predational ratee \n",
      "ance bro moriet arounce heng gapenates sevenary both moraces unter obseright dim\n",
      "querds thearry by warly least three contrickes sumber one corgeting a feacured s\n",
      "f of absh it yenggraim arbs be five note one maloghaths amaraphacance mourjand w\n",
      "righting ellan ese that bagreence of the laseamsese his calloagry an evente the \n",
      "================================================================================\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2100: 1.705511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2200: 1.676220 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2300: 1.687073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2400: 1.683289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2500: 1.699696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.676465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2700: 1.693675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2800: 1.649205 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2900: 1.657143 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3000: 1.664989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "fous by mael towal knovairy is of his goss its the bach as of overmands lew are \n",
      "versollawed the lyvels on a hishio gines trubests from gunity of the sorrited ar\n",
      " pouced the huloring the know of that kemoundoig time abekimamdation nuclificus \n",
      "and collarzly own is excessiled organing that herrisunus lonyer chulls lave scri\n",
      "not of text some of some of agains his two used of the greyanation russiuse resu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3100: 1.659535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3200: 1.650154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3300: 1.632935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3400: 1.637622 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3500: 1.627180 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3600: 1.629728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.631192 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3800: 1.624709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3900: 1.617507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4000: 1.618164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "g shorsed knite in the gathspoty was skible support referentry londoriths haven \n",
      "an resucts of incrua mavate and musicay issp or subshicrann with thesery und obs\n",
      "kilar adders musicion of the pritages new enso exactabine was the national imati\n",
      "whyphtral people in time six and two only is sco a nignes deccopeents we combuti\n",
      "zar and than an additional kaken the new and some octebour three excears deseati\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4100: 1.620780 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4200: 1.607310 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4300: 1.589188 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4400: 1.616980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4500: 1.624907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4600: 1.627506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4700: 1.599863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4800: 1.580287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4900: 1.597601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5000: 1.622665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "y in iff course amoriation suchninol hon are competwencay the longing of megings\n",
      "gened pare smiller supprosofizing chointed president later of the male considers\n",
      "h the majeme the pleadering but leym of rolinarrine common a playment imm this r\n",
      "encured a litturcants and eight the rographined and conscried of the seven foer \n",
      "wark for recognical see n two zero in seaso the light into crenjeging cailqinely\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5100: 1.635255 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.627053 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5300: 1.590392 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5400: 1.586821 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5500: 1.579422 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5600: 1.607284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5700: 1.564953 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.571926 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.588731 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6000: 1.561438 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "y the was prodic exgent games indepenten caphar apporces shueb is when ah firnt \n",
      "ugned stalte five into some which is altot decesting pali invenged stince the un\n",
      "menta and hugrican choselule to concult for marinmentation o ins the feloctionif\n",
      "ment of band xic and this relatel land when two zero zero zero s aboon acchardih\n",
      "er the was light the lim chara following state purilns in called maccies over mu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6100: 1.579980 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6200: 1.595982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6300: 1.607038 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6400: 1.634157 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6500: 1.635582 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6600: 1.600307 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6700: 1.589921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6800: 1.574565 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6900: 1.572231 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.580819 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "nors riving a anglew electio stunnives of his compirate homusstect of suncently \n",
      "ra polia gast success warmoppine part over that return betters the hetpenses or \n",
      "varres royar in the studge abstinced juffing into two zero zero zero the more ma\n",
      "poptir sinded gaiblatica without the large uswaring the government saines tradam\n",
      "quingtous myde in play film rafors insecopobing on bassical well elecondallic an\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() # grab next batch\n",
    "        feed_dict = dict() # initialize a new dictionary\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            # batches is a list of 64 arrays of 27 numbers representing letters\n",
    "            # below, you're loading that into the dictionary to feed mini-batches\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            \n",
    "        # call tensorFlow to get training predictions\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        # increment average loss?\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:]) # flattening batches into single list\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution()) # generates column of random probabilities\n",
    "                    sentence = characters(feed)[0] # finds characters according the the probs\n",
    "                    reset_sample_state.run()\n",
    "                \n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    \n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "                \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            \n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "(64, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batches[1])\n",
    "print(feed_dict[train_data[1]].shape)\n",
    "print(feed_dict[train_data[1]])\n",
    "print(labels[1:10])\n",
    "sample(random_distribution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1: define fast, simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters\n",
    "    # Gate inputs, memory, outputs consolidated to single matrix\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \n",
    "        all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        \n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        \n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "        \n",
    "        update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        \n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "    \n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        # batch_size = 64, vocabulary_size = 27\n",
    "        # so, for each \"unrolling\" we're appending 1608 length vector to train_data \n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list() # blank list object\n",
    "    output = saved_output # s_o saves state across unrollings\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state) \n",
    "        # pass lstm function the training input record, the prior output and the prior state. get update\n",
    "        outputs.append(output) # add output to the stack\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # looks like it's multiplying input (x) * weights (w) + biases (b) to get logits\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "        # running softmax cross entropy loss function using logits, train_labels\n",
    "        \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        # decaying (stepping way down (stair=True)) learning rate every 5k steps i.e., from 10 to 1\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        # passing loss function, computing gradient on loss, unzipping to provide gradients and \"v\"\n",
    "    \n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        # clipping to eliminate possible exploding gradient\n",
    "        \n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "        # re-zipping possibly corrected gradient and \"v\", applying those gradients to optimizer\n",
    "        \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "        # simple softmax of previously calculated logits\n",
    "        \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        # using the last sample output and state, softmax predict on logits from current output AND current w & b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1: run fast LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294374 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "e egf zu n ye zpsdvkox uskzdsdwzzxuggekp re csich ne ydshitko ygshiibifblhvem zl\n",
      "otrhel  eewharwcrm wm  yei f eeroyleyequzeaztve  rcdxr qrtnhatmgqk j jpzuvw hu e\n",
      "pjofgkfegrhyggwemopgzncxmvtgo hclurnzuyeoenwysyexx stti fodsigho jp  dpeisantott\n",
      "eitut oe gekgpne iijatr vccc nesul rluk tl  ve ogtei zkretr eyqjrmmihpvaur mty a\n",
      "s zl seeigh ihldn onkrfhlsf uozjeav eoictjltriuudnpqiajrte vaoeeeluwed    jadhay\n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 100: 2.596132 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.01\n",
      "Validation set perplexity: 10.39\n",
      "Average loss at step 200: 2.285682 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.21\n",
      "Validation set perplexity: 9.09\n",
      "Average loss at step 300: 2.112313 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 2.024300 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 500: 1.939075 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 600: 1.909944 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.875960 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 800: 1.854317 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.828242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1000: 1.786754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "y seligention willinfine betrems in helown the cremsora of lla is loke of vet be\n",
      "ber las boled was on the wix verent ow can infers arbowical in the deurs mano sc\n",
      "cance turle estection siven and resonin siven therows and cip the es the press a\n",
      "zer unit by the uperalar ks were zero fige the secally benferser s clash cliacte\n",
      "tidic and prosially and whive and of sixtes manding bleaplend elenson formations\n",
      "================================================================================\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1100: 1.789111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1200: 1.778194 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1300: 1.766824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1400: 1.736728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1500: 1.728240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1600: 1.714947 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1700: 1.710766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 1800: 1.722318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 1900: 1.697292 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2000: 1.702801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "x conspuble and poltuctelet in one eigner of the ory were a the for zero six mar\n",
      "j e capriction doversed ishol chaced and dame could in the eiginene the cardable\n",
      "wers or salosomy the news bundery now decout were lazake boofver of the shigner \n",
      "m of six masks appts in also one and llate obrecknourcbre elsever in to fore one\n",
      "gefrefearly in guase has a nottly one eight one nine zero zero seven that fore e\n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2100: 1.687826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2200: 1.705246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2300: 1.687792 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2400: 1.680439 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2500: 1.687795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2600: 1.673511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2700: 1.658436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2800: 1.651870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2900: 1.649050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.673222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "xigral bewwwill ance fordriat norl away by the crikes his elements britisting li\n",
      "ty terly zero vol weeks internist and vates pronsian tour of eight two ralction \n",
      "y and award from prinagilangirina minf gike filmul conitiens he priveered lenned\n",
      "ilar linder weast a ral secrous of stallyn has part cophclessor one six two zero\n",
      "grasplist of the aresures mawape pene works will reprods of experiensed thiik or\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3100: 1.641538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3200: 1.651420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3300: 1.647494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3400: 1.636609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.620644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3600: 1.637688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3700: 1.611475 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.603765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3900: 1.592726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.617233 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "cedenist the flophi zeronyalisms deptedice of the maters thee the competer one s\n",
      "ing receite in in the cacturation nevel and a end one nine eight one out bassial\n",
      "ea the compition court later with dellages in some succective in physicci and su\n",
      "k file one eight six seve five bich petelardwish a a codrice themes and an entro\n",
      "ourphisy poets of end write and presing the deuk out flomal ourger a god i clace\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.631036 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4200: 1.610699 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.572626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4400: 1.599522 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4500: 1.583679 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4600: 1.589999 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4700: 1.600449 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4800: 1.594634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.615373 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.626153 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "================================================================================\n",
      "garbot yeaturalis is no link one eight seven his fax antputy that hobants of the\n",
      "k oprocced as in less of the proce mother and of naten in the for trog years ban\n",
      "zer the alfone and reston c talshort arebhorme of as the god polite was in at cu\n",
      "justiun of the juchmo two can acat of he possease some farchoal diaclipaty three\n",
      "uss any ladeore ell resougs and while interaite in the numb one kights four from\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5100: 1.579454 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5200: 1.596780 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5300: 1.571262 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5400: 1.565926 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5500: 1.562908 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5600: 1.553482 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5700: 1.585147 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5800: 1.572610 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5900: 1.582883 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.545095 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "================================================================================\n",
      "t of centraward protengt useritagora about plutator the unoby companiel only wit\n",
      "galles over knowled creation is a first ds encigners rowalted totable bose remir\n",
      "histor for one nine nine five zero zero zero s ractionally comit in one nine eig\n",
      "per that with the criticility used the fudining two eight five low has of the po\n",
      "hish douted marchible a major quay was establions two states the binical origing\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6100: 1.590977 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200: 1.590805 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.575522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6400: 1.594398 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.597624 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6600: 1.579896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6700: 1.575646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800: 1.585967 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.617603 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 7000: 1.604791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "videring of one eight eight four in eajonials and watergr pogpences and kyrnaton\n",
      "t nethic runned the one eight five juad on jubroating known and seal supine sums\n",
      " mit dicktus tart in the larghing one one nine in one nine seven nine five tines\n",
      "ply life tachives gdstsoners cantures anicalowical busing an execuriph six zero \n",
      "zers complement opeciniphs authorposialish need the jodten admresseshings in the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() # grab next batch\n",
    "        feed_dict = dict() # initialize a new dictionary\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            # batches is a list of 64 arrays of 27 numbers representing letters\n",
    "            # below, you're loading that into the dictionary to feed mini-batches\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            \n",
    "        # call tensorFlow to get training predictions\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        # increment average loss?\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:]) # flattening batches into single list\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution()) # generates column of random probabilities\n",
    "                    sentence = characters(feed)[0] # finds characters according the the probs\n",
    "                    reset_sample_state.run()\n",
    "                \n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    \n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "                \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            \n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1: w/ beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2: bigram embedding lookup (use CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2: bigram (training) batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2: bigram sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2: bigram LSTM model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram\n",
    "# dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2: run bigram LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pn: multiple sequences / hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pn: multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "num_steps = 24001\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    " \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    " \n",
    "  saved_output2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "   \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input_w = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    " \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    " \n",
    "  # Definition of the 2nd LSTM layer\n",
    "  m_input_w2 = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle_w2 = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases2 = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    " \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    m_input = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "   \n",
    "    m_input = tf.nn.dropout(m_input, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input, m_input_w) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "   \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "   \n",
    "    return output_gate * tf.tanh(state), state\n",
    " \n",
    "  def lstm_cell_2(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "   Note that in this formulation, we omit the various connections between the\n",
    "   previous state and the gates.\"\"\"    \n",
    "    m_input2 = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output2 = tf.pack([o for _ in range(m_rows)])\n",
    "   \n",
    "    m_input2 = tf.nn.dropout(m_input2, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input2, m_input_w2) + tf.batch_matmul(m_saved_output2, m_middle_w2) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "   \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "   \n",
    "    return output_gate * tf.tanh(state), state\n",
    " \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    " \n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    " \n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    " \n",
    "  train_inputs = encoded_inputs\n",
    " \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output1 = saved_output1\n",
    "  output2 = saved_output2\n",
    "  state1 = saved_state1\n",
    "  state2 = saved_state2\n",
    "  for i in train_inputs:\n",
    "    output1, state1 = lstm_cell_improved(i, output1, state1)\n",
    "    output2, state2 = lstm_cell_2(output1, output2, state2)\n",
    "    outputs.append(output2)\n",
    " \n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output1.assign(output1),\n",
    "                                saved_state1.assign(state1),\n",
    "                                saved_output2.assign(output2),\n",
    "                                saved_state2.assign(state2)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    " \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    " \n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    " \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output2.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state2.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output1, sample_state1 = lstm_cell_improved(\n",
    "    sample_embed, saved_sample_output1, saved_sample_state1)\n",
    "  sample_output2, sample_state2 = lstm_cell_2(\n",
    "    sample_output1, saved_sample_output2, saved_sample_state2)\n",
    "  with tf.control_dependencies([saved_sample_output1.assign(sample_output1),\n",
    "                                saved_sample_state1.assign(sample_state1),\n",
    "                                saved_sample_output2.assign(sample_output2),\n",
    "                                saved_sample_state2.assign(sample_state2)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output2, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-a7c516c1b131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# batches is a list of 64 arrays of 27 numbers representing letters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# below, you're loading that into the dictionary to feed mini-batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# call tensorFlow to get training predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#num_steps = 7001\n",
    "summary_frequency = 100\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() # grab next batch\n",
    "        feed_dict = dict() # initialize a new dictionary\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            # batches is a list of 64 arrays of 27 numbers representing letters\n",
    "            # below, you're loading that into the dictionary to feed mini-batches\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            \n",
    "        # call tensorFlow to get training predictions\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        # increment average loss?\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:]) # flattening batches into single list\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution()) # generates column of random probabilities\n",
    "                    sentence = characters(feed)[0] # finds characters according the the probs\n",
    "                    reset_sample_state.run()\n",
    "                \n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    \n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "                \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            \n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
